{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMK2uP3/biNTW4PmZSlQkK9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toluolatubosun/Simple-RAG-LLM-Chatbot/blob/main/RAG_LLM_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzZBfkBZqdjT"
      },
      "outputs": [],
      "source": [
        "# Install Packages\n",
        "%pip install supabase\n",
        "%pip install boto3\n",
        "%pip install openai\n",
        "%pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the modules\n",
        "import os\n",
        "import json\n",
        "import boto3\n",
        "import supabase\n",
        "from openai import OpenAI\n",
        "from tabulate import tabulate\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "15QBxdzTqp0B"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Enviroment Variables\n",
        "\n",
        "# Locally\n",
        "# SUPABASE_URL = os.environ.get('SUPABASE_URL')\n",
        "# SUPABASE_KEY = os.environ.get('SUPABASE_KEY')\n",
        "# OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
        "\n",
        "# Google Collab\n",
        "SUPABASE_URL = userdata.get('SUPABASE_URL_2')\n",
        "SUPABASE_KEY = userdata.get('SUPABASE_KEY_2')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY_2')"
      ],
      "metadata": {
        "id": "InhEELker-jC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Global Objects and Variables\n",
        "\n",
        "# Supbase Client\n",
        "supabase_client = supabase.create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "# OpenAI\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "oZOEVeR5sS-c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faq_data = [\n",
        "    {\n",
        "        \"question\": \"What courses are offered at Mountain Top University?\",\n",
        "        \"answer\": \"Mountain Top University offers programs in sciences, social sciences, humanities, management, and technology.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Where is Mountain Top University located?\",\n",
        "        \"answer\": \"It is located at KM 12, Lagos-Ibadan Expressway, Prayer City, Ogun State, Nigeria.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Does Mountain Top University offer scholarships?\",\n",
        "        \"answer\": \"Yes, the university offers merit-based and need-based scholarships to qualified students.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the admission process for Mountain Top University?\",\n",
        "        \"answer\": \"Admission is through UTME or direct entry. Candidates must meet departmental cut-off marks and pass the screening process.\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "5DailpRAstE0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text: str, model: str = \"text-embedding-ada-002\") -> list:\n",
        "    response = openai_client.embeddings.create(\n",
        "        input=text,\n",
        "        model=model\n",
        "    )\n",
        "    return response.data[0].embedding"
      ],
      "metadata": {
        "id": "xdY0nZP7uSzn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_faq_to_supabase():\n",
        "    for item in faq_data:\n",
        "        question = item[\"question\"]\n",
        "        answer = item[\"answer\"]\n",
        "\n",
        "        question_embedding = get_embedding(question)\n",
        "        answer_embedding = get_embedding(answer)\n",
        "\n",
        "        data = {\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"question_embedding\": question_embedding,\n",
        "            \"answer_embedding\": answer_embedding\n",
        "        }\n",
        "\n",
        "        supabase_client.table(\"faq_question\").insert(data).execute()\n",
        "        print(f\"Uploaded: {question}\")\n",
        "\n",
        "upload_faq_to_supabase()"
      ],
      "metadata": {
        "id": "7_k3nvursuaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```sql\n",
        "create or replace function match_faq_questions(\n",
        "  input_embedding vector,\n",
        "  similarity_threshold float,\n",
        "  match_limit int\n",
        ")\n",
        "returns table (\n",
        "  id uuid,\n",
        "  question text,\n",
        "  answer text,\n",
        "  question_embedding vector,\n",
        "  answer_embedding vector,\n",
        "  similarity_score float\n",
        ")\n",
        "language sql\n",
        "stable\n",
        "as $$\n",
        "  select\n",
        "    fq.id,\n",
        "    fq.question,\n",
        "    fq.answer,\n",
        "    fq.question_embedding,\n",
        "    fq.answer_embedding,\n",
        "    greatest(\n",
        "      1 - (fq.question_embedding <=> input_embedding),\n",
        "      1 - (fq.answer_embedding <=> input_embedding)\n",
        "    ) as similarity_score\n",
        "  from\n",
        "    faq_questions fq\n",
        "  where\n",
        "    (1 - (fq.question_embedding <=> input_embedding)) >= similarity_threshold\n",
        "    or (1 - (fq.answer_embedding <=> input_embedding)) >= similarity_threshold\n",
        "  order by\n",
        "    similarity_score desc\n",
        "  limit match_limit;\n",
        "$$;\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Tmw9rQURt0T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_faq(text: str, print_table: bool, threshold: float = 0.75, limit: int = 5):\n",
        "    embedding = get_embedding(text)\n",
        "\n",
        "    # Call Supabase RPC function\n",
        "    response = supabase_client.rpc(\n",
        "        \"match_faq_questions\",\n",
        "        {\n",
        "            \"input_embedding\": embedding,\n",
        "            \"similarity_threshold\": threshold,\n",
        "            \"match_limit\": limit\n",
        "        }\n",
        "    ).execute()\n",
        "\n",
        "    if not response.data:\n",
        "        print(\"No matches found.\")\n",
        "        return\n",
        "\n",
        "    # Pretty print results\n",
        "    rows = []\n",
        "    for item in response.data:\n",
        "        rows.append([\n",
        "            round(item[\"similarity_score\"], 3),\n",
        "            item[\"question\"],\n",
        "            item[\"answer\"]\n",
        "        ])\n",
        "\n",
        "    if print_table:\n",
        "        print(tabulate(rows, headers=[\"Similarity\", \"Question\", \"Answer\"], tablefmt=\"fancy_grid\"))\n",
        "\n",
        "    return response.data\n",
        "\n",
        "search_faq(\"Location of MTU\", True)\n",
        "print(\"Maches gone\")"
      ],
      "metadata": {
        "id": "cKqs1kOVuavH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def terminal_chatbot():\n",
        "    print(\"💬 Welcome to the MTU FAQ Assistant. Type 'exit' to quit.\\n\")\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a helpful assistant for Mountain Top University. \"\n",
        "                \"Use the provided tool to search the FAQ database when appropriate.\"\n",
        "            )\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Define the tool\n",
        "    faq_tool = {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"fetch_faq_tool\",\n",
        "            \"description\": \"Searches MTU FAQ using a question\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"text\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"User question about Mountain Top University\"\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"text\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"👤 You: \")\n",
        "        if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"👋 Goodbye!\")\n",
        "            break\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # Call the API\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=messages,\n",
        "            tools=[faq_tool],\n",
        "            tool_choice=\"auto\"\n",
        "        )\n",
        "\n",
        "        # Process the response\n",
        "        response_message = response.choices[0].message\n",
        "        tool_calls = response_message.tool_calls\n",
        "\n",
        "        # Check if the model wants to call a tool\n",
        "        if tool_calls:\n",
        "            messages.append(response_message)  # extend conversation with assistant's reply\n",
        "\n",
        "            for tool_call in tool_calls:\n",
        "                if tool_call.function.name == \"fetch_faq_tool\":\n",
        "                    args = json.loads(tool_call.function.arguments)\n",
        "                    query_text = args[\"text\"]\n",
        "\n",
        "                    print(f\"🛠️ Tool called: fetch_faq_tool → '{query_text}'\")\n",
        "\n",
        "                    result = search_faq(query_text, print_table=False)\n",
        "                    answer = result[0][\"answer\"] if result else \"No relevant FAQ found.\"\n",
        "\n",
        "                    messages.append({\n",
        "                        \"tool_call_id\": tool_call.id,\n",
        "                        \"role\": \"tool\",\n",
        "                        \"name\": tool_call.function.name,\n",
        "                        \"content\": answer,\n",
        "                    })\n",
        "\n",
        "            # Get a new response from the model after providing tool output\n",
        "            second_response = openai_client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=messages,\n",
        "            )\n",
        "            assistant_reply = second_response.choices[0].message.content\n",
        "            print(f\"🤖 Assistant: {assistant_reply}\\n\")\n",
        "            messages.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
        "\n",
        "        else:\n",
        "            # If no tool call, the response is the final answer\n",
        "            assistant_reply = response_message.content\n",
        "            print(f\"🤖 Assistant: {assistant_reply}\\n\")\n",
        "            messages.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
        "\n",
        "await terminal_chatbot()"
      ],
      "metadata": {
        "id": "kp9x3Gy-yXLD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}